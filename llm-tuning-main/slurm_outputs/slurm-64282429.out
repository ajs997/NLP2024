Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/zwz3wu/huggingface/hub/token
Login successful
learning rate: 0.0003

Load the pre-trained model: meta-llama/Llama-2-7b-hf
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]
/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Load training data from data/MMML/out_preprocess/dev.json
Load val data from data/MMML/out_preprocess/val.json
Map:   0%|          | 0/285 [00:00<?, ? examples/s]Map:  25%|██▌       | 72/285 [00:00<00:00, 710.71 examples/s]Map:  58%|█████▊    | 166/285 [00:00<00:00, 838.94 examples/s]Map:  92%|█████████▏| 261/285 [00:00<00:00, 886.03 examples/s]Map: 100%|██████████| 285/285 [00:00<00:00, 650.42 examples/s]
Map:   0%|          | 0/1531 [00:00<?, ? examples/s]Map:   7%|▋         | 108/1531 [00:00<00:01, 1063.99 examples/s]Map:  16%|█▋        | 249/1531 [00:00<00:01, 963.44 examples/s] Map:  23%|██▎       | 347/1531 [00:00<00:01, 967.71 examples/s]Map:  32%|███▏      | 490/1531 [00:00<00:01, 956.77 examples/s]Map:  39%|███▊      | 590/1531 [00:00<00:00, 968.94 examples/s]Map:  45%|████▌     | 690/1531 [00:00<00:00, 975.09 examples/s]Map:  55%|█████▍    | 839/1531 [00:00<00:00, 979.25 examples/s]Map:  64%|██████▎   | 973/1531 [00:01<00:00, 945.05 examples/s]Map:  73%|███████▎  | 1110/1531 [00:01<00:00, 856.00 examples/s]Map:  79%|███████▊  | 1205/1531 [00:01<00:00, 874.47 examples/s]Map:  88%|████████▊ | 1347/1531 [00:01<00:00, 896.66 examples/s]Map:  95%|█████████▍| 1453/1531 [00:01<00:00, 934.63 examples/s]Map: 100%|██████████| 1531/1531 [00:01<00:00, 924.64 examples/s]
/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/9 [00:00<?, ?it/s]/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Traceback (most recent call last):
  File "/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main/demo.py", line 56, in <module>
    fire.Fire(main)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main/demo.py", line 27, in main
    m.train(
  File "/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main/lora/llm_lora_mmml.py", line 235, in train
    trainer.train()
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/transformers/trainer.py", line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/transformers/trainer.py", line 2911, in training_step
    self.accelerator.backward(loss)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/accelerate/accelerator.py", line 1999, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 480, in backward
    grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype_A)
RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float
  0%|          | 0/9 [00:02<?, ?it/s]
