Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/zwz3wu/huggingface/hub/token
Login successful
learning rate: 0.0003

Load the pre-trained model: meta-llama/Llama-2-7b-hf
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Load training data from data/MMML/out_preprocess/dev.json
Load val data from data/MMML/out_preprocess/val.json
Map:   0%|          | 0/285 [00:00<?, ? examples/s]Map:  38%|███▊      | 108/285 [00:00<00:00, 1061.87 examples/s]Map:  80%|███████▉  | 227/285 [00:00<00:00, 1133.79 examples/s]Map: 100%|██████████| 285/285 [00:00<00:00, 1048.92 examples/s]
Map:   0%|          | 0/1531 [00:00<?, ? examples/s]Map:   7%|▋         | 104/1531 [00:00<00:01, 1009.76 examples/s]Map:  14%|█▍        | 211/1531 [00:00<00:01, 1040.22 examples/s]Map:  21%|██▏       | 326/1531 [00:00<00:01, 1081.56 examples/s]Map:  32%|███▏      | 489/1531 [00:00<00:00, 1073.96 examples/s]Map:  40%|███▉      | 605/1531 [00:00<00:00, 1100.08 examples/s]Map:  50%|████▉     | 762/1531 [00:00<00:00, 1069.47 examples/s]Map:  60%|█████▉    | 917/1531 [00:00<00:00, 1051.46 examples/s]Map:  69%|██████▉   | 1054/1531 [00:01<00:00, 957.20 examples/s]Map:  76%|███████▌  | 1164/1531 [00:01<00:00, 987.54 examples/s]Map:  84%|████████▎ | 1280/1531 [00:01<00:00, 1029.50 examples/s]Map:  91%|█████████ | 1388/1531 [00:01<00:00, 1039.75 examples/s]Map:  99%|█████████▊| 1510/1531 [00:01<00:00, 1087.45 examples/s]Map: 100%|██████████| 1531/1531 [00:01<00:00, 1027.10 examples/s]
/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/9 [00:00<?, ?it/s]/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Traceback (most recent call last):
  File "/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main/demo.py", line 56, in <module>
    fire.Fire(main)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main/demo.py", line 27, in main
    m.train(
  File "/sfs/weka/scratch/zwz3wu/NLP2024/llm-tuning-main/lora/llm_lora_mmml.py", line 235, in train
    trainer.train()
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/transformers/trainer.py", line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/transformers/trainer.py", line 2911, in training_step
    self.accelerator.backward(loss)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/accelerate/accelerator.py", line 1999, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
  File "/home/zwz3wu/.conda/envs/test/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 480, in backward
    grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype_A)
RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float
  0%|          | 0/9 [00:01<?, ?it/s]
